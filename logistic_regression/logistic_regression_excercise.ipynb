{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Consulting Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Customer Churn\n",
    "\n",
    "A marketing agency has many customers that use their service to produce ads for the client/customer websites. They've noticed that they have quite a bit of churn in clients. They basically randomly assign account managers right now, but want you to create a machine learning model that will help predict which customers will churn (stop buying their service) so that they can correctly assign the customers most at risk to churn an account manager. Luckily they have some historical data, can you help them out? Create a classification algorithm that will help classify whether or not a customer churned. Then the company can test this against incoming data for future customers to predict which customers will churn and assign them an account manager.\n",
    "\n",
    "The data is saved as customer_churn.csv. Here are the fields and their definitions:\n",
    "\n",
    "    Name : Name of the latest contact at Company\n",
    "    Age: Customer Age\n",
    "    Total_Purchase: Total Ads Purchased\n",
    "    Account_Manager: Binary 0=No manager, 1= Account manager assigned\n",
    "    Years: Totaly Years as a customer\n",
    "    Num_sites: Number of websites that use the service.\n",
    "    Onboard_date: Date that the name of the latest contact was onboarded\n",
    "    Location: Client HQ Address\n",
    "    Company: Name of Client Company\n",
    "    \n",
    "Once you've created the model and evaluated it, test out the model on some new data (you can think of this almost like a hold-out set) that your client has provided, saved under new_customers.csv. The client wants to know which customers are most likely to churn given this data (they don't have the label yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>8</td><td>application_1524041113502_0012</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-yhphil.ourta4v5mcau1gb0wpnqcg2qtb.fx.internal.cloudapp.net:8088/proxy/application_1524041113502_0012/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn2-yhphil.ourta4v5mcau1gb0wpnqcg2qtb.fx.internal.cloudapp.net:30060/node/containerlogs/container_1524041113502_0012_01_000001/livy\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('customer_churn_project').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('hdfs://...customer_churn.csv',\n",
    "                    inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+--------------+---------------+-----+---------+--------------------+--------------------+--------------------+-----+\n",
      "|              Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|        Onboard_date|            Location|             Company|Churn|\n",
      "+-------------------+----+--------------+---------------+-----+---------+--------------------+--------------------+--------------------+-----+\n",
      "|   Cameron Williams|42.0|       11066.8|              0| 7.22|      8.0|2013-08-30 07:00:...|10265 Elizabeth M...|          Harvey LLC|    1|\n",
      "|      Kevin Mueller|41.0|      11916.22|              0|  6.5|     11.0|2013-08-13 00:38:...|6157 Frank Garden...|          Wilson PLC|    1|\n",
      "|        Eric Lozano|38.0|      12884.75|              0| 6.67|     12.0|2016-06-29 06:20:...|1331 Keith Court ...|Miller, Johnson a...|    1|\n",
      "|      Phillip White|42.0|       8010.76|              0| 6.71|     10.0|2014-04-22 12:43:...|13120 Daniel Moun...|           Smith Inc|    1|\n",
      "|     Cynthia Norton|37.0|       9191.58|              0| 5.56|      9.0|2016-01-19 15:31:...|765 Tricia Row Ka...|          Love-Jones|    1|\n",
      "|   Jessica Williams|48.0|      10356.02|              0| 5.12|      8.0|2009-03-03 23:13:...|6187 Olson Mounta...|        Kelly-Warren|    1|\n",
      "|        Eric Butler|44.0|      11331.58|              1| 5.23|     11.0|2016-12-05 03:35:...|4846 Savannah Roa...|   Reynolds-Sheppard|    1|\n",
      "|      Zachary Walsh|32.0|       9885.12|              1| 6.92|      9.0|2006-03-09 14:50:...|25271 Roy Express...|          Singh-Cole|    1|\n",
      "|        Ashlee Carr|43.0|       14062.6|              1| 5.46|     11.0|2011-09-29 05:47:...|3725 Caroline Str...|           Lopez PLC|    1|\n",
      "|     Jennifer Lynch|40.0|       8066.94|              1| 7.11|     11.0|2006-03-28 15:42:...|363 Sandra Lodge ...|       Reed-Martinez|    1|\n",
      "|       Paula Harris|30.0|      11575.37|              1| 5.22|      8.0|2016-11-13 13:13:...|Unit 8120 Box 916...|Briggs, Lamb and ...|    1|\n",
      "|     Bruce Phillips|45.0|       8771.02|              1| 6.64|     11.0|2015-05-28 12:14:...|Unit 1895 Box 094...|    Figueroa-Maynard|    1|\n",
      "|       Craig Garner|45.0|       8988.67|              1| 4.84|     11.0|2011-02-16 08:10:...|897 Kelley Overpa...|     Abbott-Thompson|    1|\n",
      "|       Nicole Olson|40.0|       8283.32|              1|  5.1|     13.0|2012-11-22 05:35:...|11488 Weaver Cape...|Smith, Kim and Ma...|    1|\n",
      "|     Harold Griffin|41.0|       6569.87|              1|  4.3|     11.0|2015-03-28 02:13:...|1774 Peter Row Ap...|Snyder, Lee and M...|    1|\n",
      "|       James Wright|38.0|      10494.82|              1| 6.81|     12.0|2015-07-22 08:38:...|45408 David Path ...|      Sanders-Pierce|    1|\n",
      "|      Doris Wilkins|45.0|       8213.41|              1| 7.35|     11.0|2006-09-03 06:13:...|28216 Wright Moun...|Andrews, Adams an...|    1|\n",
      "|Katherine Carpenter|43.0|      11226.88|              0| 8.08|     12.0|2006-10-22 04:42:...|Unit 4948 Box 481...|Morgan, Phillips ...|    1|\n",
      "|     Lindsay Martin|53.0|       5515.09|              0| 6.85|      8.0|2015-10-07 00:27:...|69203 Crosby Divi...|      Villanueva LLC|    1|\n",
      "|        Kathy Curry|46.0|        8046.4|              1| 5.69|      8.0|2014-11-06 23:47:...|9569 Caldwell Cre...|Berry, Orr and Ca...|    1|\n",
      "+-------------------+----+--------------+---------------+-----+---------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Churn: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Names', 'Age', 'Total_Purchase', 'Account_Manager', 'Years', 'Num_Sites', 'Onboard_date', 'Location', 'Company', 'Churn']"
     ]
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+--------------------+--------------------+-------------------+\n",
      "|summary|        Names|              Age|   Total_Purchase|   Account_Manager|            Years|         Num_Sites|            Location|             Company|              Churn|\n",
      "+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+--------------------+--------------------+-------------------+\n",
      "|  count|          900|              900|              900|               900|              900|               900|                 900|                 900|                900|\n",
      "|   mean|         null|41.81666666666667|10062.82403333334|0.4811111111111111| 5.27315555555555| 8.587777777777777|                null|                null|0.16666666666666666|\n",
      "| stddev|         null|6.127560416916251|2408.644531858096|0.4999208935073339|1.274449013194616|1.7648355920350969|                null|                null| 0.3728852122772358|\n",
      "|    min|   Aaron King|             22.0|            100.0|                 0|              1.0|               3.0|00103 Jeffrey Cre...|     Abbott-Thompson|                  0|\n",
      "|    max|Zachary Walsh|             65.0|         18026.01|                 1|             9.15|              14.0|Unit 9800 Box 287...|Zuniga, Clark and...|                  1|\n",
      "+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+--------------------+--------------------+-------------------+"
     ]
    }
   ],
   "source": [
    "# Check to see if there are any missing values. Since each category has a count of 900, there are no missing values.\n",
    "\n",
    "df.describe().show()\n",
    "\n",
    "\n",
    "# if you were to drop na values you would do code below. However, maybe better to fill in \n",
    "# values such as with column averages\n",
    "\n",
    "# final_df = df.na.drop()#.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(training_set, test_set) = df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Prep: Categorical data to boolean matrix, assemble feature vector, and make model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 steps (index then one hot encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (OneHotEncoder, StringIndexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 string index and then 2 one hot encode. \n",
    "# turns catebories into indexes like 1,2,3\n",
    "# one hot encode turns each index into a boolean matrix so only 0's and 1's\n",
    "\n",
    "location_indexer = StringIndexer(inputCol = \"Location\", outputCol= \"LocationIndex\")\n",
    "location_encoder = OneHotEncoder(inputCol=\"LocationIndex\", outputCol=\"locationvector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "company_indexer = StringIndexer(inputCol = \"Company\", outputCol= \"CompanyIndex\")\n",
    "company_encoder = OneHotEncoder(inputCol=\"CompanyIndex\", outputCol=\"companyvector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assemble Feature Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build features vector (leave out label which in this case is \"churn\")\n",
    "# we only keep columns that we want to use as features for our ML model\n",
    "\n",
    "from pyspark.ml.feature import (VectorAssembler, VectorIndexer)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"Age\",\n",
    "                                       \"Total_Purchase\",\n",
    "                                       \"Years\",\n",
    "                                       \"Num_Sites\",\n",
    "                                       \"locationvector\",\n",
    "                                       \"companyvector\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic_reg_churn = LogisticRegression(featuresCol=\"features\", labelCol=\"Churn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline: making workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = [location_indexer, \n",
    "                              company_indexer,\n",
    "                              location_encoder,\n",
    "                              company_encoder,\n",
    "                              assembler,\n",
    "                              logistic_reg_churn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a column called \"predictions\" will be created when you do this\n",
    "# you are fitting to training set and then applying that newly made model to the test set\n",
    "fitted_model_to_training_set = pipeline.fit(training_set)\n",
    "\n",
    "results = fitted_model_to_training_set.transform(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o225.showString.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 45.0 failed 4 times, most recent failure: Lost task 0.3 in stage 45.0 (TID 49, wn2-yhphil.ourta4v5mcau1gb0wpnqcg2qtb.fx.internal.cloudapp.net, executor 1): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (string) => double)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:234)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Unseen label: 325 Lawrence Crossing Suite 269 South Josephhaven, OK 53193-5178.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:170)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:166)\n",
      "\t... 16 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1928)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1941)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1954)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:336)\n",
      "\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2386)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n",
      "\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withNewExecutionId(Dataset.scala:2788)\n",
      "\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2385)\n",
      "\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2392)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2128)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2127)\n",
      "\tat org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2818)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2127)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2342)\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:248)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:280)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (string) => double)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:234)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.SparkException: Unseen label: 325 Lawrence Crossing Suite 269 South Josephhaven, OK 53193-5178.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:170)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:166)\n",
      "\t... 16 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/dataframe.py\", line 318, in show\n",
      "    print(self._jdf.showString(n, 20))\n",
      "  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\", line 319, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "Py4JJavaError: An error occurred while calling o225.showString.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 45.0 failed 4 times, most recent failure: Lost task 0.3 in stage 45.0 (TID 49, wn2-yhphil.ourta4v5mcau1gb0wpnqcg2qtb.fx.internal.cloudapp.net, executor 1): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (string) => double)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:234)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Unseen label: 325 Lawrence Crossing Suite 269 South Josephhaven, OK 53193-5178.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:170)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:166)\n",
      "\t... 16 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1928)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1941)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1954)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:336)\n",
      "\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2386)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n",
      "\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withNewExecutionId(Dataset.scala:2788)\n",
      "\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2385)\n",
      "\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2392)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2128)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2127)\n",
      "\tat org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2818)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2127)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2342)\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:248)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:280)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (string) => double)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:234)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.SparkException: Unseen label: 325 Lawrence Crossing Suite 269 South Josephhaven, OK 53193-5178.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:170)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:166)\n",
      "\t... 16 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "\n",
    "# ERROR, maybe because you \"locations\" and \"company\" are NOT really categories. Almost all of them are unique!\n",
    "results.select(\"Churn\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Something is wrong. Try taking \"location\" and \"company\" out since most of them are NOT unique anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (VectorAssembler, VectorIndexer)\n",
    "\n",
    "assembler_revised = VectorAssembler(inputCols=[\"Age\",\n",
    "                                               \"Total_Purchase\",\n",
    "                                               \"Years\",\n",
    "                                               \"Num_Sites\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline_revised = Pipeline(stages = [assembler_revised, logistic_reg_churn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train on training_set and apply to test set\n",
    "\n",
    "trained_model = pipeline_revised.fit(training_set)\n",
    "\n",
    "results_revised = trained_model.transform(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+\n",
      "|Churn|         probability|prediction|\n",
      "+-----+--------------------+----------+\n",
      "|    0|[0.99743276331492...|       0.0|\n",
      "|    0|[0.91271280232428...|       0.0|\n",
      "|    0|[0.97940753647492...|       0.0|\n",
      "|    0|[0.98873180213160...|       0.0|\n",
      "|    0|[0.33926499966210...|       1.0|\n",
      "|    0|[0.78679030003224...|       0.0|\n",
      "|    0|[0.92445602289555...|       0.0|\n",
      "|    0|[0.99282424486168...|       0.0|\n",
      "|    1|[0.65837238759989...|       0.0|\n",
      "|    0|[0.93879882893471...|       0.0|\n",
      "|    0|[0.55007777911216...|       0.0|\n",
      "|    0|[0.98930496405472...|       0.0|\n",
      "|    0|[0.95317477930104...|       0.0|\n",
      "|    0|[0.60449203169551...|       0.0|\n",
      "|    1|[0.84797385831219...|       0.0|\n",
      "|    0|[0.99306649752273...|       0.0|\n",
      "|    0|[0.94846416196978...|       0.0|\n",
      "|    0|[0.97637470604408...|       0.0|\n",
      "|    1|[0.12430520506404...|       1.0|\n",
      "|    0|[0.98934070130561...|       0.0|\n",
      "+-----+--------------------+----------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "results_revised.select(\"Churn\", \"probability\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Results_Revised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import (BinaryClassificationEvaluator, \n",
    "                                   MulticlassClassificationEvaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluation = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",\n",
    "                                          labelCol=\"Churn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Area Under Curve evaluating\n",
    "\n",
    "AUC = evaluation.evaluate(results_revised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7026932318782539"
     ]
    }
   ],
   "source": [
    "AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model on New Customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is more like a real test set. The previous \"test\" set is more like a Cross-Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "re_test_df = spark.read.csv(\"wasb:///sparkcourse/data/new_customers.csv\",\n",
    "                            inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "# NOTICE THAT THERE IS NO CHURN COLUMN\n",
    "\n",
    "re_test_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# apparently, no need to select only the features you are working with for the model to work. \n",
    "# You can provide the whole dataset and the model will only use what it was trained to use and it\n",
    "# will also make a \"feature\" vector. So, you don't need to assemble a feature vector (see table below)\n",
    "\n",
    "new_customer_results = trained_model.transform(re_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+--------------+---------------+-----+---------+--------------------+--------------------+----------------+--------------------+--------------------+--------------------+----------+\n",
      "|         Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|        Onboard_date|            Location|         Company|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------+----+--------------+---------------+-----+---------+--------------------+--------------------+----------------+--------------------+--------------------+--------------------+----------+\n",
      "| Andrew Mccall|37.0|       9935.53|              1| 7.71|      8.0|2011-08-29 18:37:...|38612 Johnny Stra...|        King Ltd|[37.0,9935.53,7.7...|[2.44641938853100...|[0.92029921300875...|       0.0|\n",
      "|Michele Wright|23.0|       7526.94|              1| 9.28|     15.0|2013-07-22 18:19:...|21083 Nicole Junc...|   Cannon-Benson|[23.0,7526.94,9.2...|[-5.4186041014652...|[0.00441376313846...|       1.0|\n",
      "|  Jeremy Chang|65.0|         100.0|              1|  1.0|     15.0|2006-12-11 07:48:...|085 Austin Views ...|Barron-Robertson|[65.0,100.0,1.0,1...|[-2.9576346874026...|[0.04937691283449...|       1.0|\n",
      "|Megan Ferguson|32.0|        6487.5|              0|  9.4|     14.0|2016-10-28 05:32:...|922 Wright Branch...|   Sexton-Golden|[32.0,6487.5,9.4,...|[-4.6905146544091...|[0.00909841809851...|       1.0|\n",
      "|  Taylor Young|32.0|      13147.71|              1| 10.0|      8.0|2012-03-20 00:36:...|Unit 0789 Box 073...|        Wood LLC|[32.0,13147.71,10...|[1.41318830142924...|[0.80426833547650...|       0.0|\n",
      "| Jessica Drake|22.0|       8445.26|              1| 3.46|     14.0|2011-02-04 19:29:...|1148 Tina Straven...|   Parks-Robbins|[22.0,8445.26,3.4...|[-1.5542524002344...|[0.17447293780169...|       1.0|\n",
      "+--------------+----+--------------+---------------+-----+---------+--------------------+--------------------+----------------+--------------------+--------------------+--------------------+----------+"
     ]
    }
   ],
   "source": [
    "new_customer_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+----------+\n",
      "|         Company|         probability|prediction|\n",
      "+----------------+--------------------+----------+\n",
      "|        King Ltd|[0.92029921300875...|       0.0|\n",
      "|   Cannon-Benson|[0.00441376313846...|       1.0|\n",
      "|Barron-Robertson|[0.04937691283449...|       1.0|\n",
      "|   Sexton-Golden|[0.00909841809851...|       1.0|\n",
      "|        Wood LLC|[0.80426833547650...|       0.0|\n",
      "|   Parks-Robbins|[0.17447293780169...|       1.0|\n",
      "+----------------+--------------------+----------+"
     ]
    }
   ],
   "source": [
    "new_customer_results.select(\"Company\", \"probability\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+------------------+-----------------+------------------+-----------------+------------------+--------------------+----------------+------------------+\n",
      "|summary|        Names|               Age|   Total_Purchase|   Account_Manager|            Years|         Num_Sites|            Location|         Company|        prediction|\n",
      "+-------+-------------+------------------+-----------------+------------------+-----------------+------------------+--------------------+----------------+------------------+\n",
      "|  count|            6|                 6|                6|                 6|                6|                 6|                   6|               6|                 6|\n",
      "|   mean|         null|35.166666666666664|7607.156666666667|0.8333333333333334|6.808333333333334|12.333333333333334|                null|            null|0.6666666666666666|\n",
      "| stddev|         null| 15.71517313511584|4346.008232825459| 0.408248290463863|3.708737880555414|3.3862466931200785|                null|            null|0.5163977794943223|\n",
      "|    min|Andrew Mccall|              22.0|            100.0|                 0|              1.0|               8.0|085 Austin Views ...|Barron-Robertson|               0.0|\n",
      "|    max| Taylor Young|              65.0|         13147.71|                 1|             10.0|              15.0|Unit 0789 Box 073...|        Wood LLC|               1.0|\n",
      "+-------+-------------+------------------+-----------------+------------------+-----------------+------------------+--------------------+----------------+------------------+"
     ]
    }
   ],
   "source": [
    "new_customer_results.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Client Companies likely to churn from new customers list according to logistic regression model: Cannon-Benson, Barron, Barron-Robertson, Sexton-Golden, and Park-Robbins."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}